<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>How to self-host a Python package index using Pulp</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/01/17/how-self-host-python-package-index-using-pulp" /><author><name>Fridolin Pokorny, Ina Panova, Fabricio Aguiar, Harshad Reddy Nalla</name></author><id>06502559-fb92-41a5-ac8e-9f942b759601</id><updated>2022-01-17T07:00:00Z</updated><published>2022-01-17T07:00:00Z</published><summary type="html">&lt;p&gt;Every &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; developer or data scientist gets to the point where they need to consume, and often publish, Python packages. The main source of open source, publicly available Python packages is &lt;a href="https://pypi.org/"&gt;PyPI&lt;/a&gt;, which is used worldwide. PyPI, also known as &lt;a href="https://wiki.python.org/moin/CheeseShop"&gt;CheeseShop&lt;/a&gt;, hosts 3 million Python package releases as of this writing. In some cases, however, your team might need to host a Python package index internally. This article introduces &lt;a href="https://pulpproject.org/"&gt;Pulp&lt;/a&gt;, an open source project for managing repositories of software packages. Our example shows how the Pulp instance works on the &lt;a href="https://www.operate-first.cloud/"&gt;Operate First&lt;/a&gt; environment where it is hosted. Our example is based on how data scientists and Python developers at Red Hat use the Operate First deployment.&lt;/p&gt; &lt;h2&gt;Managing software repositories with Pulp&lt;/h2&gt; &lt;p&gt;&lt;a href="https://pulpproject.org/"&gt;Pulp&lt;/a&gt; can manage content in various formats: RPM packages, &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; images, &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Ansible&lt;/a&gt; roles, &lt;a href="https://maven.apache.org/"&gt;Maven&lt;/a&gt; repositories, Python package indexes, and others. See Pulp's &lt;a href="https://pulpproject.org/content-plugins/"&gt;content plugins documentation&lt;/a&gt; for a full listing.&lt;/p&gt; &lt;p&gt;From a Python packaging perspective, you are likely most interested in the &lt;a href="https://docs.pulpproject.org/pulp_python/"&gt;Python content plugin&lt;/a&gt;, which you can use to create and host multiple Python package indexes on a single Pulp instance. This perfectly fits into a scenario where multiple teams wish to manage their own Python package index, but may need to operate just one Pulp instance (or very few instances) deployed within the organization.&lt;/p&gt; &lt;p&gt;Because Pulp is supported by Red Hat engineers and is modular, our teams within Red Hat decided to use Pulp to host our Python packages. The &lt;a href="https://www.operate-first.cloud/community-handbook/pulp/usage.md"&gt;Pulp Python package index&lt;/a&gt; is deployed in the &lt;a href="https://www.operate-first.cloud/"&gt;Operate First&lt;/a&gt; production environment. We'll use that as our example for using a Pulp instance as a Python package index.&lt;/p&gt; &lt;h2&gt;How to use the Pulp Python package index&lt;/h2&gt; &lt;p&gt;The documentation at the &lt;a href="https://www.operate-first.cloud/community-handbook/pulp/usage.md"&gt;Operate First index's site&lt;/a&gt; walks you through setting up a Python package index, publishing Python packages, and consuming already hosted Python packages from the Pulp Python package index. Let's look at the main functions of using Pulp.&lt;/p&gt; &lt;h3&gt;Setting up a Pulp Python repository&lt;/h3&gt; &lt;p&gt;To set up a repository, &lt;a href="https://github.com/operate-first/support/issues/new/choose#:~:text=User%20support%3A%20New%20Python%20package%20index%20request"&gt;submit a request to the Operate First support team&lt;/a&gt;, as shown in Figure 1. After your request is processed, the instance and the access to it will be configured and ready for use.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/request_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/request_0.png?itok=yJagsA4p" width="1280" height="895" alt="Submit a request for a new repository." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Submit a request for a new repository. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Publishing Python packages&lt;/h3&gt; &lt;p&gt;After your private index has been set up, you can publish Python packages there. Currently, you need to follow the steps documented in Project Thoth's &lt;a href="https://github.com/thoth-station/aicoe-ci-pulp-upload-example"&gt;hello world example application&lt;/a&gt;. Eventually, we hope that role-based access control (RBAC) will be enabled.&lt;/p&gt; &lt;h3&gt;Consuming Python packages from a Pulp Python package index&lt;/h3&gt; &lt;p&gt;With a simple command, you can consume the packages hosted on the Operate First cloud:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ pip install --index-url "https://pulp.operate-first.cloud/pypi/&lt;index-name&gt;/simple/" --extra-index-url "https://pypi.org/simple"&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; By including the &lt;a href="https://pip.pypa.io/en/stable/cli/pip_install/#cmdoption-extra-index-url"&gt;--extra-index-url&lt;/a&gt; option, you can ask pip to fall back on PyPI to retrieve packages that are not found on the specified private index.&lt;/p&gt; &lt;h2&gt;Acknowledgments&lt;/h2&gt; &lt;p&gt;The Pulp instance on the Operate First environment is live and available to developers after 10 months of cross-team collaboration between engineers from the &lt;a href="https://pulpproject.org/"&gt;Pulp team&lt;/a&gt;, the &lt;a href="https://thoth-station.ninja"&gt;Project Thoth&lt;/a&gt; team, the team supporting the &lt;a href="https://www.operate-first.cloud/"&gt;Operate First&lt;/a&gt; deployments, and Python engineers who were involved during the process.&lt;/p&gt; &lt;p&gt;We would like to thank everyone who was part of this effort. Thanks, especially, to the following engineers who were actively involved in the collaboration:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Bob Fahr, Insights-core team&lt;/li&gt; &lt;li&gt;Brian Gollaher, Red Hat Enterprise Linux Product Management&lt;/li&gt; &lt;li&gt;Chris Hambridge, Ansible Engineering&lt;/li&gt; &lt;li&gt;Christoph Goern, Project Thoth&lt;/li&gt; &lt;li&gt;Christian Heimes, Red Hat Identity Management, CPython upstream, Python Packaging Authority&lt;/li&gt; &lt;li&gt;Daniel Alley, Pulp project&lt;/li&gt; &lt;li&gt;Gerrod Ubben, Pulp project&lt;/li&gt; &lt;li&gt;Pavel Tisnovsky, Connected Customer Experience (CCX)&lt;/li&gt; &lt;li&gt;Sviatoslav Sydorenko, Ansible Core Engineering, Python Packaging Authority&lt;/li&gt; &lt;li&gt;Tomas Orsava, Python maintenance team&lt;/li&gt; &lt;li&gt;Tom Coufal, Open Services Team&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/01/17/how-self-host-python-package-index-using-pulp" title="How to self-host a Python package index using Pulp"&gt;How to self-host a Python package index using Pulp&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Fridolin Pokorny, Ina Panova, Fabricio Aguiar, Harshad Reddy Nalla</dc:creator><dc:date>2022-01-17T07:00:00Z</dc:date></entry><entry><title type="html">Red Hat Summit 2022 - Talking Architecture Shop series</title><link rel="alternate" href="http://www.schabell.org/2022/01/red-hat-summit-2022-talking-architecture-shop-series.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2022/01/red-hat-summit-2022-talking-architecture-shop-series.html</id><updated>2022-01-17T06:00:00Z</updated><content type="html">  It's that time of year again when we get the call for papers! I heard the call and thought this year would be a perfect time to go all in with sessions around our architectures based on a series of talks we've designed to showcase the various aspects we cover. Some are vertical aligned and others are just customer domains, but all of them include extensive research into how to implement successful architectures at scale. Last week the call for papers closed and the internal committees are starting to churn through all of the excellent content they've been given. Sometime in March 2022 we should hear back from them. How about a look at the various sessions we've put together where I'll be presenting with my good friends in the architecture team? We've started a series called Talking Architecture Shop to host our various architectural explorations and they've been well received so far. For Red Hat Summit 2022 we've pushed the following into the selection process, with co-speakers listed and the name of the theme submitted into. TALKING ARCHITECTURE SHOP - EXPLORING OPEN SOURCE SUCCESS AT SCALE You've heard of large scale open source architectures, but have you ever wanted to take a serious look at these real life enterprise implementations that scale? This session takes attendees on a tour of multiple use cases covering enterprise challenges like integration, optimisation, cloud adoption, hybrid cloud management, healthcare, retail, manufacturing, financial services, and much more. Not only are these architectures interesting, but they are successful real life implementations featuring open source technologies and power many of your own online experiences. The attendee departs this session with a working knowledge of how to map general open source technologies to their solutions. Material covered is available freely online and attendees can use these solutions as starting points for aligning to their own solution architectures. Join us for an hour of power as we talk architecture shop! (Digital Transformation) TALKING ARCHITECTURE SHOP - EXPLORING OPEN SOURCE EDGE SUCCESS AT SCALE You've heard of large scale open source architectures, but have you ever wanted to take a serious look at real life enterprise edge implementations that scale? This session takes attendees on a tour of multiple use cases for enterprise challenges on the edge with integration, telco, healthcare, manufacturing, and much more. Not only are these architectures interesting, but they are successful real life implementations featuring open source technologies and power many of your own edge experiences. The attendee departs this session with a working knowledge of how to map general open source technologies to their own edge solutions. Material covered is available freely online and attendees can use these solutions as starting points for aligning to their own solution architectures. Join us for an hour of power as we talk architecture shop! (Emerging Technology) (with ) TALKING ARCHITECTURE SHOP - EXPLORING OPEN SOURCE GITOPS SUCCESS AT SCALE You've heard of large scale open source architectures, but have you ever wanted to take a serious look at these real life enterprise GitOps implementations that scale? This session takes attendees on a tour of multiple use cases for enterprise challenges using GitOps with integration, optimisation, hybrid cloud management, healthcare, and much more. Not only are these architectures interesting, but they are successful real life implementations featuring open source technologies and power many of your own operations and development experiences. The attendee departs this session with a working knowledge of how to map general open source technologies to their own GitOps solutions. Material covered is available freely online and attendees can use these solutions as starting points for aligning to their own solution architectures. Join us for an hour of power as we talk architecture shop! (Developers) (with ) TALKING ARCHITECTURE SHOP - EXPLORING OPEN SOURCE SAP SUCCESS AT SCALE You've heard of large scale open source SAP architectures, but have you ever wanted to take a serious look at these real life enterprise implementations that scale? This session takes attendees on a tour of multiple use cases covering enterprise challenges with SAP around integration, optimisation, cloud adoption, and much more. Not only are these architectures interesting, but they are successful real life implementations featuring open source technologies and power many of your own online experiences. The attendee departs this session with a working knowledge of how to map general open source technologies for their SAP architectural solutions. Material covered is available freely online and attendees can use these solutions as starting points for aligning to their own solution architectures. Join us for an hour of power as we talk architecture shop! (Digital Transformation) (with ) TALKING ARCHITECTURE SHOP - EXPLORING OPEN SOURCE TELCO SUCCESS AT SCALE You've heard of large scale open source architectures, but have you ever wanted to take a serious look at real life enterprise telco implementations that scale? This session takes attendees on a tour of multiple use cases for enterprise challenges in the telco domain with a look at use cases for integration, networks, 5G, hyperscalers, and much more. Not only are these architectures interesting, but they are successful real life implementations featuring open source technologies and power many of your own edge experiences. The attendee departs this session with a working knowledge of how to map general open source technologies to their own telco solutions. Material covered is available freely online and attendees can use these solutions as starting points for aligning to their own solution architectures. Join us for an hour of power as we talk architecture shop! (Edge)  (with ) TALKING ARCHITECTURE SHOP - EXPLORING OPEN SOURCE PLATFORM SUCCESS AT SCALE You've heard of large scale open source architectures, but have you ever wanted to take a serious look at real life enterprise platform implementations that scale? This session takes attendees on a tour of multiple use cases for enterprise challenges focused on platform use cases covering remote server management, cloud adoption, financial services, self-healing infrastructure, and much more. Not only are these architectures interesting, but they are successful real life implementations featuring open source technologies and power many of your own experiences. The attendee departs this session with a working knowledge of how to map general open source technologies to their own platform solutions. Material covered is available freely online and attendees can use these solutions as starting points for aligning to their own solution architectures. Join us for an hour of power as we talk architecture shop! (Platforms) (with ) TALKING ARCHITECTURE SHOP - EXPLORING CLOUD SERVICES SUCCESS AT SCALE You've heard of large scale open source architectures, but have you ever wanted to take a serious look at real life enterprise cloud services implementations that scale? This session takes attendees on a tour of multiple use cases for enterprise challenges focused on cloud services within use cases covering integration, healthcare, financial services, cloud services development, and much more. Not only are these architectures interesting, but they are successful real life implementations featuring open source technologies and power many of your own cloud services experiences. The attendee departs this session with a working knowledge of how to map general open source technologies to their own cloud services solutions. Material covered is available freely online and attendees can use these solutions as starting points for aligning to their own solution architectures. Join us for an hour of power as we talk architecture shop! (Cloud Services) (with ) Here's hoping that we'll be having a face to face session in 2022 at Red Hat Summit and talking architecture shop!</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title>Extracting dependencies from Python packages</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/01/14/extracting-dependencies-python-packages" /><author><name>Fridolin Pokorny</name></author><id>91fcccf3-b847-401b-89a1-6073c5d82959</id><updated>2022-01-14T07:00:00Z</updated><published>2022-01-14T07:00:00Z</published><summary type="html">&lt;p&gt;Python's easy-to-learn syntax and rich standard library, combined with the large number of &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; software packages available on the &lt;a href="https://pypi.org/"&gt;Python Package Index&lt;/a&gt; (PyPI), make it a common programming language of choice for quick prototyping leading to production systems. &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; is a good fit for many use cases, and is particularly popular in the &lt;a href="https://developers.redhat.com/topics/data-science"&gt;data science&lt;/a&gt; domain for data exploration and analysis.&lt;/p&gt; &lt;p&gt;Thus, Python's rapid rise on the &lt;a href="https://www.tiobe.com/tiobe-index/python/"&gt;TIOBE Index&lt;/a&gt; of the most popular programming languages shouldn't be a surprise. PyPI hosts more than 3 million releases of Python packages. Each package release has metadata associated with it, which makes the packages themselves an interesting dataset to explore and experiment with.&lt;/p&gt; &lt;p&gt;In this article, you'll learn how to extract metadata and dependency information from Python package releases. You'll also see how this process works in &lt;a href="https://thoth-station.ninja/"&gt;Project Thoth&lt;/a&gt;, which provides Python programmers with information about support for the packages they use, along with the dependencies, performance, and security of those packages.&lt;/p&gt; &lt;h2&gt;Python package releases and PyPI&lt;/h2&gt; &lt;p&gt;The bar chart in Figure 1 shows the number of Python package releases on PyPI from March 2005 to mid-July 2021, with each bar representing one month. As you can see, the number of package releases is growing more or less exponentially.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-thoth-resolver-pypi_growth-fig1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-thoth-resolver-pypi_growth-fig1.png?itok=Dw6ht_LK" width="600" height="598" alt="A chart showing Python package releases available on PyPI from March 2005 until mid-July 2021. Each bar represents the number of Python package releases available per month." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The number of Python package releases available on PyPI from March 2005 until mid-July 2021. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;As the Python Package Index's name suggests, it really is &lt;a href="https://pretalx.com/packagingcon-2021/talk/X8N8ME/"&gt;an index of software packages&lt;/a&gt; (as an example, see the &lt;a href="https://pypi.org/simple/flask/"&gt;links for Flask releases&lt;/a&gt;). A simple artifact listing has its pros and cons. One of the advantages is easy artifact serving for self-hosted Python package indexes or mirrors. If you provide a simple HTTP server with exposed content conforming to the &lt;a href="https://www.python.org/dev/peps/pep-0503/"&gt;Simple Repository API&lt;/a&gt; (Python Enhancement Proposal 503), then all the Python client tooling, such as &lt;a href="https://github.com/pypa/pip"&gt;pip&lt;/a&gt;, will be automatically able to use your self-hosted Python package indexes and install packages from your server. A downside of this approach is the lack of additional package metadata, especially dependency information.&lt;/p&gt; &lt;h2&gt;Why collecting Python dependency information is challenging&lt;/h2&gt; &lt;p&gt;Dustin Ingram, a PyPI maintainer, wrote about the challenges of collecting Python dependency information in &lt;a href="https://dustingram.com/articles/2018/03/05/why-pypi-doesnt-know-dependencies"&gt;Why PyPI doesn't know your project's dependencies&lt;/a&gt;. In short, Python's source distributions execute code that is supposed to provide information about dependencies at installation time. Because the dependency listing is not provided statically, but is a result of arbitrary code execution, dependencies can be specific to the installation-script logic. This allows for computing dependencies at installation time and gives the power to express dependencies dynamically. On the other hand, the behavior is generally unpredictable and can cause headaches when trying to obtain dependency information for a package release.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; Dependencies are generally computed based on the runtime environment where the installation process executes arbitrary code. As a result, the installation can be used by malicious Python package releases to &lt;a href="https://developers.redhat.com/articles/2021/12/21/prevent-python-dependency-confusion-attacks-thoth"&gt;steal environment information or perform other malicious actions at installation time&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Recent changes to Python packaging standards have shifted away from providing dependency information during installation, and toward exposing it statically in built &lt;a href="https://www.python.org/dev/peps/pep-0427"&gt;wheels&lt;/a&gt; (PEP 427). Newer Python package releases often follow this trend, but Python packaging and tooling also tries to be backward compatible as much as possible. For a more in-depth explanation, see &lt;a href="https://pretalx.com/packagingcon-2021/talk/X8N8ME"&gt;Python packaging: Why don’t you just...?&lt;/a&gt;, a presentation from Tzu-ping Chung, one of the Python package maintainers.&lt;/p&gt; &lt;h2&gt;How Thoth collects dependency information&lt;/h2&gt; &lt;p&gt;Python artifacts specific to a Python package release can provide multiple builds besides source distributions. These builds target different environments and respect Python's &lt;a href="https://www.python.org/dev/peps/pep-0425"&gt;packaging tags for built distributions&lt;/a&gt; (PEP 425). It's up to pip (or whatever installer you choose) to select the correct built distribution for the environment in which the installer is running. These tags can specify ABI, platform, or other requirements for the target environment, as discussed in the PEP 425 documentation. If none of the built distributions match the target environment, the installer can fall back to installing the release from source distributions if provided. This process might involve additional requirements for the target environment, such as a compatible build toolchain if source distributions require building native extensions.&lt;/p&gt; &lt;p&gt;To streamline the whole process, &lt;a href="https://thoth-station.ninja/"&gt;Project Thoth&lt;/a&gt; offers a component that re-uses the logic that performs these actions in pip. This component, &lt;a href="https://github.com/thoth-station/solver"&gt;thoth-solver&lt;/a&gt;, is written as a Python application that is primarily designed to run in &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized&lt;/a&gt; environments. The thoth-solver component installs Python packages in the specified version from the desired Python package index, by letting pip decide which Python artifact should be installed into the environment where thoth-solver runs. This naturally can involve triggering package builds out of source distributions as necessary. Once the package is installed using pip's logic, thoth-solver extracts metadata out of the installed artifact, together with additional information about the thoth-solver run itself.&lt;/p&gt; &lt;p&gt;The result is a JSON document containing information about the artifact together with the environment in which the solver runs, Python-specific entries (such as hashes of files), and &lt;a href="https://packaging.python.org/en/latest/specifications/core-metadata/"&gt;Python's core metadata&lt;/a&gt;. It may also include additional dependency information, such as details about version range specifications, versions matching version range specifications of dependencies, extras, or environment markers, along with evaluation results specifically tailored for the containerized environment (see &lt;a href="https://www.python.org/dev/peps/pep-0508/"&gt;PEP 508 for more information&lt;/a&gt;). Thoth can obtain this information from multiple Python package indexes that host artifacts analyzed by thoth-solver as well as dependencies for artifacts hosted on other indexes (for example, &lt;a href="https://tensorflow.pypi.thoth-station.ninja/"&gt;AVX2-enabled builds of TensorFlow hosted on the AI Center of Excellence index&lt;/a&gt;). The procedure and data aggregated allow Thoth to check how packages form dependencies across different Python package indexes for cross-index Python package resolution.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; If a given package is not installable into the containerized environment (due to incompatibilities between Python 2 and 3, or a missing build toolchain, for example), thoth-solver reports information about the failure that can be further post-processed to extract relevant details and classify the error.&lt;/p&gt; &lt;p&gt;To see how thoth-solver works in practice, take a look at this &lt;a href="https://gist.github.com/fridex/48aa44a7a348f63da068a5174b48eb1b"&gt;example output from a thoth-solver run for Flask in version 2.0.2 available from PyPI&lt;/a&gt;. The result gives information about dependencies for &lt;a href="https://pypi.org/project/Flask/2.0.2/"&gt;flask==2.0.2&lt;/a&gt; when installed into a containerized &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat Universal Base Image&lt;/a&gt; &lt;a href="https://developers.redhat.com/products/rhel/download"&gt;Red Hat Enterprise Linux 8&lt;/a&gt; environment running Python 3.8 at the given point in time. The containerized environment is available on Quay as &lt;a href="https://quay.io/repository/thoth-station/solver-rhel-8-py38"&gt;solver-rhel-8-py38&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Using thoth-solver&lt;/h2&gt; &lt;p&gt;The thoth-solver component is part of Project Thoth's cloud-based Python resolver. It aggregates information about dependencies in &lt;a href="https://developers.redhat.com/blog/2021/04/26/continuous-learning-in-project-thoth-using-kafka-and-argo"&gt;Thoth's background data aggregation&lt;/a&gt; and makes them available for &lt;a href="https://developers.redhat.com/articles/2021/11/17/customize-python-dependency-resolution-machine-learning"&gt;Thoth's resolver&lt;/a&gt;. The Thoth team provides &lt;a href="https://github.com/thoth-station/solver/tree/master/overlays"&gt;multiple thoth-solver containerized environments&lt;/a&gt;, built container images of which are &lt;a href="https://quay.io/organization/thoth-station"&gt;available on Quay&lt;/a&gt;. These compute dependency information specifically for their &lt;em&gt;target environment&lt;/em&gt;—a reproducible environment with a predefined software stack—for each desired Python package release individually.&lt;/p&gt; &lt;p&gt;Keep in mind that the computed dependency information is specific to the particular point in time when thoth-solver is run. As packages get new releases, another component in Thoth—the &lt;a href="https://github.com/thoth-station/revsolver"&gt;revsolver&lt;/a&gt;, or "reverse solver"—can keep the dependency information up to date. The revsolver component uses data that has already been computed by thoth-solver and is available in a queryable form in Thoth's database. In this case, revsolver does not download any artifacts, but instead uses an already captured dependency graph available to propagate information about a new package release, which becomes part of the updated ecosystem's dependency graph available in the database.&lt;/p&gt; &lt;h2&gt;About Project Thoth&lt;/h2&gt; &lt;p&gt;As part of Project Thoth, we are accumulating knowledge to help Python developers create healthy applications. If you would like to follow updates, feel free to subscribe to our &lt;a href="https://www.youtube.com/channel/UClUIDuq_hQ6vlzmqM59B2Lw"&gt;YouTube channel&lt;/a&gt; or follow us on the &lt;a href="https://twitter.com/thothstation"&gt;@ThothStation&lt;/a&gt; Twitter handle.&lt;/p&gt; &lt;p&gt;To send us feedback or get involved in improving the Python ecosystem, please contact the Thoth Station &lt;a href="https://github.com/thoth-station/support"&gt;support repository&lt;/a&gt;. You can also directly reach out to the Thoth team on Twitter. You can report any issues you've spotted in open source Python libraries to the support repository or &lt;a href="https://thoth-station.ninja/docs/developers/adviser/prescription.html"&gt;directly write prescriptions for the resolver&lt;/a&gt; and send them to our &lt;a href="https://github.com/thoth-station/prescriptions/"&gt;prescriptions repository&lt;/a&gt;. By participating in these ways, you can help the Python cloud-based resolver come up with better recommendations.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/01/14/extracting-dependencies-python-packages" title="Extracting dependencies from Python packages"&gt;Extracting dependencies from Python packages&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Fridolin Pokorny</dc:creator><dc:date>2022-01-14T07:00:00Z</dc:date></entry><entry><title type="html">RESTEasy 6.0.0 Released</title><link rel="alternate" href="https://resteasy.github.io/2022/01/13/resteasy-6.0.0-release/" /><author><name /></author><id>https://resteasy.github.io/2022/01/13/resteasy-6.0.0-release/</id><updated>2022-01-13T18:11:11Z</updated><dc:creator /></entry><entry><title type="html">How to configure Web applications request limits in WildFly</title><link rel="alternate" href="http://www.mastertheboss.com/web/jboss-web-server/how-to-configure-web-applications-request-limits-in-wildfly/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-configure-web-applications-request-limits-in-wildfly" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/web/jboss-web-server/how-to-configure-web-applications-request-limits-in-wildfly/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-configure-web-applications-request-limits-in-wildfly</id><updated>2022-01-13T14:55:14Z</updated><content type="html">In this article you will learn which strategies you can adopt on WildFly application server to configure the maximum number of concurrent requests using either a programmatic approach (MicroProfile Fault Tolerance) or declarative one (Undertow configuration). Implementing a policy to define the number of concurrent requests is crucial to limit requests and prevent faults from ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>A developer's guide to CI/CD and GitOps with Jenkins Pipelines</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/01/13/developers-guide-cicd-and-gitops-jenkins-pipelines" /><author><name>Bob Reselman</name></author><id>19d5abe2-3efb-49c1-91ad-8752f9686426</id><updated>2022-01-13T07:00:00Z</updated><published>2022-01-13T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt;, or continuous integration and continuous delivery, is an essential part of the modern software development life cycle. Coupled with &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt;, CI/CD allows developers to release high-quality software almost as soon as they commit code to a repository such as GitHub.&lt;/p&gt; &lt;p&gt;Automation is a key factor for implementing effective CI/CD. In this process, developers and release engineers create scripts that have all the instructions needed to test the code in a source code repository before putting it into a production environment. The process is efficient but complex. Fortunately, there are many tools that lessen the burden.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.jenkins.io"&gt;Jenkins&lt;/a&gt; is one of the most popular tools used for CI/CD. Jenkins has been around for years and has undergone numerous revisions, adding features all along the way. One of the most transformative features added to Jenkins is the ability to run &lt;a href="https://www.jenkins.io/doc/book/pipeline/"&gt;Jenkins Pipeline jobs&lt;/a&gt; driven by an automation script stored in a Jenkinsfile. Developers and release engineers can use Jenkinsfiles to combine the practices of CI/CD and GitOps into a unified deployment process. That's the focus of this article.&lt;/p&gt; &lt;p&gt;We'll start with a brief refresher of what Jenkins is and how it applies to both CI/CD and GitOps. Then, I’ll guide you through how to use a Jenkinsfile to create deployments that combine CI/CD and GitOps.&lt;/p&gt; &lt;h2&gt;How Jenkins supports CI/CD&lt;/h2&gt; &lt;p&gt;Jenkins is an open source tool for managing deployment processes, which can range from a single task—such as running a unit test against source code—to a complex deployment process embodying many tasks. From its first release, Jenkins allowed companies to standardize their deployment process: Once a job was configured on the Jenkins server, that job could run repeatedly in the same manner according to its configuration. The developer defined the tasks to run and when to run them, and Jenkins did the rest.&lt;/p&gt; &lt;p&gt;Early releases of Jenkins required developers to define their deployment processes manually, using the Jenkins Dashboard. Moreover, each job was specific to the particular Jenkins server. Deployments (&lt;em&gt;aka&lt;/em&gt; jobs) were not easy to update or transfer among servers. If developers wanted to update a particular job, they had to go to the server’s Jenkins Dashboard and manually implement the update. And, if developers or sysadmins wanted to move a job to another Jenkins server, they had to get into the file system of the Jenkins server and copy particular directories to other target Jenkins servers. The process was laborious, particularly if the job in question was large and contained many details.&lt;/p&gt; &lt;p&gt;Fortunately, the new Jenkins Pipeline job feature addresses these drawbacks head-on.&lt;/p&gt; &lt;h2&gt;Integrating CI/CD and GitOps with Jenkinsfiles&lt;/h2&gt; &lt;p&gt;A &lt;em&gt;Jenkinsfile&lt;/em&gt; is a text file, written in the &lt;a href="https://groovy-lang.org/"&gt;Groovy&lt;/a&gt; programming language, that defines each step in a Jenkins job. Usually, a Jenkinsfile is created by a developer or system administrator with a detailed understanding of how to deploy the particular component or application.&lt;/p&gt; &lt;p&gt;Once a Jenkinsfile is created, it is committed to a repository in the version control system that’s hosting the source code. After the Jenkinsfile is committed, the developer or sysadmin creates a job in Jenkins that declares the location of the Jenkinsfile in the source code repository and instructs Jenkins when to execute the Jenkinsfile. That’s it. There is no extensive twiddling with configuration settings in the user interface (UI). The Jenkinsfile has all the instructions required to run the job. (See Figure 1.)&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/file.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/file.png?itok=4mn91ZA_" width="883" height="307" alt="The Jenkinsfile describing an application’s deployment process is imported from a source control management system and executed by the Jenkins server." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The Jenkinsfile is imported from a source control management system (SCM) and executed by the Jenkins server. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Using a Jenkinsfile makes transferring jobs between servers much easier. All that’s required is to spin up a new job in Jenkins, bind that job to the Jenkinsfile that’s stored in version control, and then declare when the job is supposed to run. All the details and intelligence about the deployment are defined in the Jenkinsfile.&lt;/p&gt; &lt;p&gt;For all intents and purposes, the Jenkinsfile is the single source of truth (SSOT) about how a deployment executes. And that SSOT is hosted in the version control repository. Putting the SSOT in a version control repository is emblematic of the GitOps way of doing things, so let's talk about that next.&lt;/p&gt; &lt;h2&gt;GitOps and the single source of truth&lt;/h2&gt; &lt;p&gt;In a GitOps-driven deployment process, all activities emanate from the version-controlled code repository. Some companies drive their GitOps deployment process directly within the particular code repository service, such as GitHub, BitBucket, or Gitlab. Other companies have an external agent such as Jenkins execute the deployment process.&lt;/p&gt; &lt;p&gt;Regardless of the approach you choose, the important thing to understand about GitOps is that the single source of truth for all deployment activity is the code repository. Using a Jenkinsfile that’s hosted in a repository to define a job that runs under Jenkins fits well with the GitOps sensibility.&lt;/p&gt; &lt;p&gt;Now that we’ve covered how Jenkins implements CI/CD, and how it fits into the GitOps way of doing things, let’s move to a concrete example. Over the next few sections, we will implement a CI/CD process by running a Jenkinsfile under a Jenkins Pipeline job.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Are you curious about how &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; and Jenkins Pipelines work together? We recommend &lt;a href="https://cloud.redhat.com/blog/jenkins-pipelines"&gt;Simply Explained: OpenShift and Jenkins Pipelines&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Deploy a Node.js application using a Jenkinsfile&lt;/h2&gt; &lt;p&gt;Recall that a Jenkinsfile is a text file that describes the details of a job that will run under Jenkins. This section presents a job that executes a three-stage deployment to build, test, and release a &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; application:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The build stage gets the application source code from GitHub and installs the dependency packages.&lt;/li&gt; &lt;li&gt;The test stage tests the application.&lt;/li&gt; &lt;li&gt;The release stage encapsulates the application into a Docker image that is then stored in a local container repository.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The following Jenkinsfile does the work of creating the local container repository. Once the container image is stored in the container repository, the Jenkinsfile runs a Docker container from the stored container image:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;node { env.NODEJS_HOME = "${tool 'basic_node'}" // on linux / mac env.PATH="${env.NODEJS_HOME}/bin:${env.PATH}" sh 'npm --version' } pipeline { agent any stages { stage('build') { steps { git branch: 'main', url: 'https://github.com/reselbob/secret-agent.git' sh "npm install" } } stage('test') { steps { script { env.SECRET_AGENT_PORT = "3060" echo "SECRET_AGENT_PORT is '${SECRET_AGENT_PORT}'" } sh "npm test" } } stage('release') { steps { script { env.SECRET_AGENT_PORT = "3050" echo "SECRET_AGENT_PORT is '${SECRET_AGENT_PORT}'" } // If the local registry container does not exists, create it sh """ if ! [ \$(docker ps --format '{{.Names}}' | grep -w registry &amp;&gt; /dev/null) ]; then docker run -d --network='host' -p 5000:5000 --restart=always --name registry registry:2; fi; """ // if the secret_agent container is running, delete it in order to create a new one sh """ if [ \$(docker ps --format '{{.Names}}' | grep -w secret_agent &amp;&gt; /dev/null) ]; then docker rm -f secret_agent; fi; """ sh "docker build -t secretagent:v1 . " sh "docker tag secretagent:v1 localhost:5000/secretagent:v1 " sh "docker run -d --network='host' -p 3050:3050 --name secret_agent localhost:5000/secretagent:v1 " sh "echo 'Secret Agent up and running on port 3050' " } } } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the Jenkinsfile has two root-level sections: &lt;code&gt;node&lt;/code&gt; and &lt;code&gt;pipeline&lt;/code&gt;. We'll look at these next.&lt;/p&gt; &lt;h3&gt;The node section of the Jenkinsfile&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;node&lt;/code&gt; section is the first step in the deployment process. It establishes a workspace in the Jenkins server under which a deployment runs. The workspace runs a Node.js application.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; The word &lt;em&gt;node&lt;/em&gt; has two different meanings here. Used by itself in a Jenkinsfile, &lt;code&gt;node&lt;/code&gt; describes the workspace. When used in the variable &lt;code&gt;basic_node&lt;/code&gt;, it refers to the Node.js runtime environment.&lt;/p&gt; &lt;p&gt;In this example of the &lt;code&gt;node&lt;/code&gt; section, the Jenkinsfile adds the location of the Node.js package to the environment’s PATH and then executes &lt;code&gt;npm --version&lt;/code&gt; to verify that the Node.js package manager is installed and accessible globally. By implication, a successful &lt;code&gt;npm --version&lt;/code&gt; command demonstrates that Node.js is installed.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;npm --version&lt;/code&gt; command is executed as a parameter to the &lt;code&gt;sh&lt;/code&gt; command. The &lt;code&gt;sh&lt;/code&gt; command is used in the Jenkinsfile to execute commands a developer typically runs in a terminal window at the command line.&lt;/p&gt; &lt;h3&gt;The pipeline section of the Jenkinsfile&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;pipeline&lt;/code&gt; section that follows the &lt;code&gt;node&lt;/code&gt; section defines the deployment that executes in three stages: build, test, and release. Each stage has a &lt;code&gt;steps&lt;/code&gt; subsection that describes the tasks that need to be executed in that stage.&lt;/p&gt; &lt;h4&gt;Build, test, and release&lt;/h4&gt; &lt;p&gt;The &lt;code&gt;build&lt;/code&gt; stage clones the application from a source code repository and installs the Node.js dependency packages that the application requires.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;test&lt;/code&gt; stage executes a &lt;code&gt;sh&lt;/code&gt; command that, in turn, runs a &lt;code&gt;npm test&lt;/code&gt; command that is special to the application. Executing &lt;code&gt;npm test&lt;/code&gt; runs the various tests that are defined within the Node.js application’s source code.&lt;/p&gt; &lt;p&gt;The instructions for the release stage are a bit more complicated. The first thing the release stage does is check whether the local container registry is running. If the local registry does not exist, the Jenkinsfile creates it.&lt;/p&gt; &lt;h4&gt;Conditional commands&lt;/h4&gt; &lt;p&gt;The next set of commands checks whether the container that will be created from the source code is already running. The container’s name is &lt;code&gt;secret_agent&lt;/code&gt;. If the container is running, the Jenkinsfile deletes it. This is done so that the Jenkins job can run repeatedly. If a second run of the Jenkins job were to encounter a running instance of the &lt;code&gt;secret_agent&lt;/code&gt; container, the job would fail. Thus, any existing &lt;code&gt;secret_agent&lt;/code&gt; container needs to be deleted.&lt;/p&gt; &lt;p&gt;Once all the conditional commands have been executed, the Jenkinsfile builds a Docker image for the &lt;code&gt;secret_agent&lt;/code&gt; code and pushes the image into the local registry. Then, an instance of the &lt;code&gt;secret_agent&lt;/code&gt; container is created using the image stored in the local container registry.&lt;/p&gt; &lt;p&gt;The important thing to note about the deployment process is that all the instructions relevant to running a job under Jenkins are defined in the Jenkinsfile. If you ever need to change any deployment instructions, you don’t need to fiddle around with the Jenkins UI. All you need to do is alter the Jenkinsfile. Isolating this work to the Jenkinsfile makes it easier to manage and audit changes to the deployment process.&lt;/p&gt; &lt;p&gt;Now we've covered the Jenkinsfile and its sections. The last thing you need to do is bind the Jenkinsfile to a Jenkins job. This is done from within the Jenkins UI, as described in the next section.&lt;/p&gt; &lt;h2&gt;Binding a Jenkinsfile to a Jenkins Pipeline job&lt;/h2&gt; &lt;p&gt;Binding a Jenkinsfile to a Jenkins Pipeline job is straightforward, as shown by the sequence of screens in Figure 2. Here are the steps:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Create a job in the Jenkins Dashboard.&lt;/li&gt; &lt;li&gt;Name the job.&lt;/li&gt; &lt;li&gt;Declare it a Jenkins Pipeline job.&lt;/li&gt; &lt;/ol&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/project.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/project.png?itok=OKu71eA5" width="1125" height="684" alt="On the Jenkins Dashboard, you create and name a job and mark it as a Jenkins Pipeline." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. On the Jenkins Dashboard, you create and name a job and mark it as a Jenkins Pipeline. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Once the Jenkins Pipeline job is created, it needs to be configured. Figure 3 shows the first two steps in the process:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Enter a description for the job.&lt;/li&gt; &lt;li&gt;Declare how often to check the source code repository for changes.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;In Figure 3, the job is configured to poll the source code repository every 15 minutes.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/config_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/config_0.png?itok=8N-WC_CY" width="1068" height="569" alt="After entering a description, choose a build trigger." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. After entering a description, choose a build trigger. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once the description and polling interval are declared, complete the configuration that binds the Jenkinsfile to the job. The following steps are illustrated in Figure 4:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Select &lt;strong&gt;Pipeline script from SCM&lt;/strong&gt; from the first dropdown menu.&lt;/li&gt; &lt;li&gt;Select &lt;strong&gt;Git&lt;/strong&gt; from the next dropdown menu. Doing this reveals an additional set of text boxes where you can do the following: &lt;ul&gt;&lt;li&gt;Enter the URL of the version code repository that has the Jenkinsfile.&lt;/li&gt; &lt;li&gt;Enter the branch that has the version of the Jenkinsfile of interest.&lt;/li&gt; &lt;li&gt;Declare the name of the Jenkinsfile.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 4 uses the default file, simply named &lt;code&gt;Jenkinsfile&lt;/code&gt;.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/job.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/job.png?itok=YTb8ARTK" width="973" height="688" alt="Configure the Jenkins Pipeline job with the URL and branch of the repository and the the name of the Jenkinsfile." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. Configure the Jenkins Pipeline job with the URL and branch of the repository and the name of the Jenkinsfile. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can have many alternative Jenkinsfiles, with names such as &lt;code&gt;Jenkinsfile_k8s&lt;/code&gt; or &lt;code&gt;Jenkinsfile_windows&lt;/code&gt;. This means that you can use the same source code repository for a variety of jobs. Each job will execute its own build instructions described by the relevant Jenkinsfile.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; The versatility that comes with using many different versions of Jenkinsfiles is particularly useful for working in application environments that have complex provisioning and configuration requirements. For example, &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; and OpenShift configurations support a wide variety of settings—everything from container configuration to security rules. Putting the provisioning and configuration settings in one Jenkinsfile can become a maintenance headache. But, splitting up configurations among many files in a way that is particular to each environment makes deployment management a lot easier. The savings in labor alone can be significant.&lt;/p&gt; &lt;p&gt;Once the Jenkins Pipeline job is configured, you can run it from the Jenkins Dashboard. As shown in Figure 5, select the Jenkins Pipeline job, then click the &lt;strong&gt;Build Now&lt;/strong&gt; menu button to run it. The job runs and the result of each stage is shown in the job’s page on the Jenkins Dashboard.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/result.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/result.png?itok=rtSL3o7z" width="1315" height="812" alt="The results of executing a Jenkinsfile are shown in the job's Jenkins Dashboard page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5. The results of executing a Jenkinsfile are shown in the job's Jenkins Dashboard page. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The beauty of it all is that the job's runtime details are described in the associated Jenkinsfile. Should the deployment process need to change, all you need to do is change the Jenkinsfile in the source code repository.&lt;/p&gt; &lt;h2&gt;The benefits of using Jenkins Pipelines for GitOps and CI/CD&lt;/h2&gt; &lt;p&gt;There are many benefits to using Jenkins Pipeline jobs in conjunction with a version control server such as GitHub, and thus combining CI/CD and GitOps.&lt;/p&gt; &lt;p&gt;First, putting deployment instructions in a central version control repository makes the repository a central location, which can be used as a single source of truth for all deployment activities. Using the repository as a single source of truth provides reliable change management and auditing capabilities. Version control and access security are also built into the service.&lt;/p&gt; &lt;p&gt;Second, declaring the build process in a Jenkinsfile makes it easier to automate deployment. You don’t have to fiddle with a UI to get your programs out; you can just write the code and let the Jenkins Pipeline do the rest.&lt;/p&gt; &lt;p&gt;Finally, Jenkins is a well-known CI/CD tool. It has all the features that are required for implementing a viable GitOps-focused deployment process. For companies already using Jenkins, making the leap to GitOps using a Jenkinsfile is much easier than starting from scratch. And for companies not yet using Jenkins, the learning curve is acceptable. This technology has proven itself over the years, and there are many &lt;a href="https://developers.redhat.com/courses/gitops"&gt;learning resources and examples&lt;/a&gt; developers can use to get up to speed.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; has a lot to offer developers who want to automate their deployment processes using scripts that are stored as a single source of truth in a version control management system. Such control and reliability are compelling reasons to make the move.&lt;/p&gt; &lt;p&gt;You can't adopt GitOps in a day or two. It takes time to get the organizational processes in place. But many developers and their organizations will find that the time required to get GitOps working under a Jenkins Pipeline job, using a Jenkinsfile, is a good investment, especially for the benefits at hand.&lt;/p&gt; &lt;h2&gt;Learn more about GitOps and CI/CD on Red Hat Developer&lt;/h2&gt; &lt;p&gt;Visit the following articles and resources for more about using GitOps and CI/CD for secure, automated deployments:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Red Hat Developer learning courses: &lt;a href="https://developers.redhat.com/courses/gitops"&gt;Develop with GitOps&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2021/05/13/why-should-developers-care-about-gitops"&gt;Why should developers care about GitOps?&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/06/29/how-apply-machine-learning-gitops"&gt;How to apply machine learning to GitOps&lt;/a&gt;&lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/08/03/managing-gitops-control-planes-secure-gitops-practices"&gt;Managing GitOps control planes for secure GitOps practices&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/06/21/modern-fortune-teller-using-gitops-automate-application-deployment-red-hat"&gt;Modern Fortune Teller: Using GitOps to automate application deployment on Red Hat OpenShift&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/01/13/developers-guide-cicd-and-gitops-jenkins-pipelines" title="A developer's guide to CI/CD and GitOps with Jenkins Pipelines"&gt;A developer's guide to CI/CD and GitOps with Jenkins Pipelines&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bob Reselman</dc:creator><dc:date>2022-01-13T07:00:00Z</dc:date></entry><entry><title>Creating a Quarkus extension for AWS CloudWatch</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-aws-cloudwatch_extension/&#xA;            " /><author><name>Bennet Schulz (https://twitter.com/bennetelli)</name></author><id>https://quarkus.io/blog/quarkus-aws-cloudwatch_extension/</id><updated>2022-01-13T00:00:00Z</updated><published>2022-01-13T00:00:00Z</published><summary type="html">Creating a Quarkus extension for AWS CloudWatch We recently had the situation that we wanted to log our Quarkus application logs to AWS CloudWatch. Basically it takes some time but is not a big deal. Adding a CloudWatch dependency, creating a Log Handler and push the logs to CloudWatch via...</summary><dc:creator>Bennet Schulz (https://twitter.com/bennetelli)</dc:creator><dc:date>2022-01-13T00:00:00Z</dc:date></entry><entry><title>Integrate Apache ActiveMQ brokers using Camel K</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/01/12/integrate-apache-activemq-brokers-using-camel-k" /><author><name>Maz Arslan, Anton Giertli</name></author><id>09666143-7ea3-489e-a486-4929d1682e5d</id><updated>2022-01-12T07:00:00Z</updated><published>2022-01-12T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://activemq.apache.org"&gt;Apache ActiveMQ&lt;/a&gt; is a highly popular message broker that features persistence, guaranteed message delivery, and high throughput. &lt;a href="https://activemq.apache.org/components/artemis/"&gt;Apache ActiveMQ Artemis&lt;/a&gt; streamlines the classic message broker implementation for &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; architectures. This article is for developers transitioning from &lt;a href="https://activemq.apache.org/components/classic/"&gt;ActiveMQ Classic&lt;/a&gt; to &lt;a href="https://activemq.apache.org/components/artemis/"&gt;ActiveMQ Artemis&lt;/a&gt;. We will show you how to get the two versions working together using &lt;a href="https://camel.apache.org/camel-k/1.7.x/index.html"&gt;Apache Camel K&lt;/a&gt;. Our example is based on &lt;a href="https://developers.redhat.com/products/amq/overview"&gt;Red Hat AMQ&lt;/a&gt; versions 6 and 7, and we will perform the steps on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift 4&lt;/a&gt;. Our code is written in &lt;a href="https://developers.redhat.com/enterprise-java"&gt;Java&lt;/a&gt;. The integration process and techniques should be applicable to many other scenarios.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; See &lt;a href="https://developers.redhat.com/articles/2021/06/30/implementing-apache-activemq-style-broker-meshes-apache-artemis"&gt;Implementing Apache ActiveMQ-style broker meshes with Apache Artemis&lt;/a&gt; for a discussion of the differences between Red Hat AMQ 6 and 7.&lt;/p&gt; &lt;h2&gt;The Camel K integration workflow&lt;/h2&gt; &lt;p&gt;Camel K is an integration framework that allows developers to exchange data easily between many common data processing tools. The demonstration in this article creates two ActiveMQ brokers, one using Red Hat AMQ 6 and the other using Red Hat AMQ 7. We then create two Camel K integrations: One to help AMQ 7 consume messages, and another to help AMQ 6 produce them. Finally, we create a third integration that functions as a message bridge between the two brokers.&lt;/p&gt; &lt;p&gt;Figure 1 illustrates the Camel K integration workflow.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/AMQSSL_CAMELK%20Diagram.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/AMQSSL_CAMELK%20Diagram.png?itok=ygU8KNpA" width="960" height="540" alt="Figure 1. A message bridge connects two ActiveMQ brokers." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1. A message bridge connects two ActiveMQ brokers.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can retrieve the files for the demonstration from the &lt;a href="https://github.com/MazArslan/CamelK-SSL-Demo"&gt;GitHub repository&lt;/a&gt; associated with this article.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;This article assumes you have a basic knowledge of the OpenShift &lt;code&gt;oc&lt;/code&gt; command-line interface and OpenShift web console. You need a running instance of OpenShift 4 and admin access on the OpenShift cluster. You will also need to &lt;a href="https://camel.apache.org/download/"&gt;install Camel K&lt;/a&gt; on your local system.&lt;/p&gt; &lt;h2&gt;Set up the cluster&lt;/h2&gt; &lt;p&gt;Create a namespace on your OpenShift instance:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-project camelk-ssl&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As the cluster administrator, install the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_integration/2020-q4/html/deploying_camel_k_integrations_on_openshift/installing-camel-k"&gt;Red Hat Camel K Operator&lt;/a&gt; and &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.7/html-single/deploying_amq_broker_on_openshift/index"&gt;Red Hat AMQ 7.7 Operator&lt;/a&gt; on your OpenShift instance from the OpenShift OperatorHub.&lt;/p&gt; &lt;p&gt;Now, create certificates to enable secure SSL communication. Make sure you set the common name &lt;code&gt;CN&lt;/code&gt; on the broker to your server domain name:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; export CLIENT_KEYSTORE_PASSWORD=password export CLIENT_TRUSTSTORE_PASSWORD=password export BROKER_KEYSTORE_PASSWORD=password export BROKER_TRUSTSTORE_PASSWORD=password #Client Keystore keytool -genkey -alias client -keyalg RSA -keystore client.ks -storepass $CLIENT_KEYSTORE_PASSWORD -keypass $CLIENT_KEYSTORE_PASSWORD -dname "CN=camelssl-example, O=RedHat, C=UK" #Broker Keystore keytool -genkey -alias broker -keyalg RSA -keystore broker.ks -storepass $BROKER_KEYSTORE_PASSWORD -keypass $BROKER_KEYSTORE_PASSWORD -dname "CN=*.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com, O=RedHat, C=UK" #Export Client PublicKey keytool -export -alias client -keystore client.ks -storepass $CLIENT_KEYSTORE_PASSWORD -file client.cert #Export Server PublicKey keytool -export -alias broker -keystore broker.ks -storepass $BROKER_KEYSTORE_PASSWORD -file broker.cert #Import Server PublicKey into Client Truststore keytool -import -alias broker -keystore client.ts -file broker.cert -storepass $CLIENT_TRUSTSTORE_PASSWORD -trustcacerts -noprompt #Import Client PublicKey into Server Truststore keytool -import -alias client -keystore broker.ts -file client.cert -storepass $BROKER_TRUSTSTORE_PASSWORD -trustcacerts -noprompt #Import Server PublicKey into Server Truststore (i.e.: trusts its self) keytool -import -alias broker -keystore broker.ts -file broker.cert -storepass $BROKER_TRUSTSTORE_PASSWORD -trustcacerts -noprompt &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Set up the ActiveMQ consumer (AMQ 7)&lt;/h2&gt; &lt;p&gt;Create a secret for the broker keystore and truststore for AMQ 7:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret generic example-amq-secret --from-file=broker.ks --from-literal=keyStorePassword=password --from-file=client.ts=broker.ts --from-literal=trustStorePassword=password&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create an AMQ 7 broker using the AMQ broker operator:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; apiVersion: broker.amq.io/v2alpha4 kind: ActiveMQArtemis metadata: name: example-amq application: example-amq namespace: camelk-ssl spec: deploymentPlan: size: 1 image: registry.redhat.io/amq7/amq-broker:7.6 requireLogin: false adminUser: admin adminPassword: admin console: expose: true acceptors: - name: amqp protocols: amqp port: 5672 sslEnabled: true sslSecret: example-amq-secret verifyHost: false expose: true &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a new AMQ broker address. This configuration also creates a route and service for the broker:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: broker.amq.io/v2alpha2 kind: ActiveMQArtemisAddress metadata: name: example-testaddress spec: addressName: test queueName: test routingType: anycast &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To test your AMQ 7 broker, you have to download &lt;a href="https://activemq.apache.org/components/artemis/download/"&gt;ActiveMQ Artemis&lt;/a&gt;. Once the download is completed, extract it and open the &lt;code&gt;/bin&lt;/code&gt; folder, where you will find the Apache Artemis script that is used for testing. You can produce and consume messages using the new AMQ 7 broker from your local machine; just make sure to change the URL and truststore location.&lt;/p&gt; &lt;p&gt;Here is a sample command to produce messages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./artemis producer --url 'amqps://example-amq7-amqp-0-svc-rte-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443?jms.username=admin&amp;jms.password=admin&amp;transport.trustStoreLocation=/home/marslan/work/camelkssl/client.ts&amp;transport.trustStorePassword=password&amp;transport.verifyHost=false' --threads 1 --protocol amqp --message-count 10 --destination 'queue://test'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is a sample command to consume messages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$./artemis consumer --url 'amqps://example-amq7-amqp-0-svc-rte-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443?jms.username=admin&amp;jms.password=admin&amp;transport.trustStoreLocation=/home/marslan/work/camelkssl/client.ts&amp;transport.trustStorePassword=password&amp;transport.verifyHost=false' --threads 1 --protocol amqp --message-count 10 --destination 'queue://test'&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Set up the ActiveMQ producer (AMQ 6)&lt;/h2&gt; &lt;p&gt;Next, create a secret for the broker keystore and truststore for AMQ 6:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret generic example-amq6-secret --from-file=broker.ks --from-literal=keyStorePassword=password --from-file=broker.ts --from-literal=trustStorePassword=password &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create an AMQ 6 broker from the command line:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-app amq63-ssl -p APPLICATION_NAME=amq6-broker -p MQ_QUEUES=test -p MQ_TOPICS=test -p MQ_USERNAME=admin \ -p MQ_PASSWORD=admin -p ActiveMQ_SECRET=example-amq6-secret -p AMQ_TRUSTSTORE=broker.ts -p AMQ_TRUSTSTORE_PASSWORD=password \ -p AMQ_KEYSTORE=broker.ks -p AMQ_KEYSTORE_PASSWORD=password -n camelk-ssl&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a route for the broker in the OpenShift console, using the &lt;code&gt;amq6-broker-tcp-ssl&lt;/code&gt; service.&lt;/p&gt; &lt;p&gt;To test your AMQ 6 broker, you can download &lt;a href="https://activemq.apache.org/components/classic/download/"&gt;ActiveMQ&lt;/a&gt;. Once the download is completed, extract it and open the &lt;code&gt;/bin&lt;/code&gt; folder, where you will find the ActiveMQ script that is used for testing. You can produce and consume messages using the new AMQ 6 broker from your local machine. Just make sure to change the URL and truststore location.&lt;/p&gt; &lt;p&gt;Enter the following command to produce messages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./activemq producer \ -Djavax.net.ssl.trustStore=/home/marslan/work/camelkssl/client.ts \ -Djavax.net.ssl.trustStorePassword=password \ -Djavax.net.ssl.keyStore=/home/marslan/work/camelkssl/client.ks \ -Djavax.net.ssl.keyStorePassword=password \ --brokerUrl ssl://test-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443 \ --user admin \ --password admin \ --destination queue://test \ --messageCount 1000 \ --message HelloWorld &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Enter the following command to consume messages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./activemq consumer \ -Djavax.net.ssl.trustStore=/home/marslan/work/camelkssl/client.ts \ -Djavax.net.ssl.trustStorePassword=password \ -Djavax.net.ssl.keyStore=/home/marslan/work/camelkssl/client.ks \ -Djavax.net.ssl.keyStorePassword=password \ --brokerUrl ssl://test-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443 \ --user admin \ --password admin \ --destination queue://test &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Test Camel K&lt;/h2&gt; &lt;p&gt;Now, we will import a simple Java application and use it to ensure that Camel K is working correctly. The &lt;code&gt;HelloCamelK.java&lt;/code&gt; program is available in the &lt;a href="https://github.com/MazArslan/CamelK-SSL-Demo"&gt;Camel K SSL Demo repository&lt;/a&gt; on GitHub.&lt;/p&gt; &lt;p&gt;Run the file with the following command (you may delete this integration after the test):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kamel run HelloCamelK&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create the AMQ 7 consumer&lt;/h2&gt; &lt;p&gt;Create folders for AMQ 6 and AMQ 7:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir amq6 $ mkdir amq7&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a new configuration directory for AMQ 7:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir amq7/configs&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the configuration directory, create an &lt;code&gt;application.properties&lt;/code&gt; file for the AMQ 7 integration, with the following contents:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;quarkus.qpid-jms.url=amqps://example-amq7-amqp-0-svc-rte-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443?transport.keyStoreLocation=/etc/ssl/example-amq-secret/broker.ks&amp;transport.keyStorePassword=password&amp;transport.trustStoreLocation=/etc/ssl/example-amq-secret/client.ts&amp;transport.trustStorePassword=password&amp;transport.verifyHost=false quarkus.qpid-jms.username=admin quarkus.qpid-jms.password=admin jms.destinationType=queue jms.destinationName=test &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Within the &lt;code&gt;amq7&lt;/code&gt; folder, create a program named &lt;code&gt;amq7consumer.java&lt;/code&gt; for the AMQ 7 consumer integration:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;// camel-k: language=java &lt;1&gt; // camel-k: dependency=mvn:org.amqphub.quarkus:quarkus-qpid-jms &lt;2&gt; import org.apache.camel.builder.RouteBuilder; public class amq7consumer extends RouteBuilder { @Override public void configure() throws Exception { from("jms:{{jms.destinationType}}:{{jms.destinationName}}").to("log:info"); &lt;3&gt; } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's look at a few parts of the code:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;camel-k: language=java&lt;/code&gt; provides information to Camel K to run the Java file. The comment enables Camel K to sort out dependencies.&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-k: dependency=mvn:org.amqphub.quarkus:quarkus-qpid-jms&lt;/code&gt; supports out-of-the-box configuration for SSL, without requiring any further customization. You can find more information in the &lt;a href="https://qpid.apache.org/releases/qpid-jms-0.54.0/docs/index.html#ssl-transport-configuration-options"&gt;Apache SSL documentation&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;The code consumes the &lt;code&gt;jms.url&lt;/code&gt;, &lt;code&gt;jms.destinationType&lt;/code&gt;, and &lt;code&gt;jms.destinationName&lt;/code&gt; properties specified in the &lt;code&gt;application.properties&lt;/code&gt; file and prints the values to the log.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The Java file is imported in the next step, where we run Camel K:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kamel run -n camelk-ssl --property file:./amq7/configs/application.properties --resource secret:example-amq-secret@/etc/ssl/example-amq-secret amq7/amq7consumer.java &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the &lt;code&gt;kamel run&lt;/code&gt; command:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The &lt;code&gt;--property file&lt;/code&gt; option imports properties to Camel.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;--resource&lt;/code&gt; option adds resources to the cluster pod. In this command, we are adding &lt;code&gt;example-amq-secret&lt;/code&gt; and placing its contents into the &lt;code&gt;/etc/ssl/example-amq-secret&lt;/code&gt; folder within the pod.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To debug Camel K, add the following option to the &lt;code&gt;kamel run&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;--trait jvm.options=-Djavax.net.debug=all&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create the AMQ 6 producer&lt;/h2&gt; &lt;p&gt;Create a new configuration directory for AMQ 6:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir amq6/configs&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the configuration directory, create an &lt;code&gt;application.properties&lt;/code&gt; file for the AMQ 6 integration, with the following contents:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;activemq.destination.brokerURL=ssl://test-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443 activemq.destination.username=admin activemq.destination.password=admin activemq.destination.ssl.keyStorePassword=password activemq.destination.ssl.keyStoreLocation=/etc/ssl/example-amq6-secret/broker.ks activemq.destination.ssl.trustStorePassword=password activemq.destination.ssl.trustStoreLocation=/etc/ssl/example-amq6-secret/broker.ts activemq.destination.type=queue activemq.destination.name=test &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Within the &lt;code&gt;amq6&lt;/code&gt; folder, create a program named &lt;code&gt;amq6SSLproducer.java&lt;/code&gt; for the AMQ 6 producer integration:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;// camel-k: dependency=camel:camel-quarkus-activemq // camel-k: dependency=camel:camel-quarkus-timer // camel-k: property=period=5000 public class amq6SSLProducer extends RouteBuilder { @Override public void configure() throws Exception { from("timer:foo?fixedRate=true&amp;period={{period}}").bean(this, "generateFakePerson()").to("log:info") .to("activemq:{{activemq.destination.type}}:{{activemq.destination.name}}?connectionFactory=#pooledConnectionFactory"); } public String generateFakePerson() { Faker faker = new Faker(); return faker.name().fullName() + " lives on " + faker.address().streetAddress(); } @ApplicationScoped public ActiveMQComponent activeMq(PooledConnectionFactory pooledConnectionFactory) { ActiveMQComponent activeMqComponent = new ActiveMQComponent(); activeMqComponent.setConnectionFactory(pooledConnectionFactory); activeMqComponent.setCacheLevelName("CACHE_CONSUMER"); return activeMqComponent; } @BindToRegistry public PooledConnectionFactory pooledConnectionFactory() throws Exception { return new PooledConnectionFactory(sslConnectionFactory()); } private ActiveMQSslConnectionFactory sslConnectionFactory() throws Exception { ActiveMQSslConnectionFactory connectionFactory = new ActiveMQSslConnectionFactory(); logger.info("BrokerURL: " + destinationBrokerURL); connectionFactory.setBrokerURL(destinationBrokerURL); connectionFactory.setUserName(destinationUserName); connectionFactory.setPassword(destinationPassword); connectionFactory.setTrustStore(destinationTrustStoreLocation); connectionFactory.setTrustStorePassword(destinationTruststorePassword); connectionFactory.setKeyStore(destinationKeyStoreLocation); connectionFactory.setKeyStorePassword(destinationKeystorePassword); return connectionFactory; } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the producer using Camel K:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kamel run -n camelk-ssl \ --property file:./amq6/configs/application.properties \ --resource secret:example-amq6-secret@/etc/ssl/example-amq6-secret amq6/amqssl.java&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create a message bridge between AMQ 6 and AMQ 7&lt;/h2&gt; &lt;p&gt;Create new folders for the message bridge and its configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir message-bridge $ mkdir message-bridge/configs&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the configuration directory, create an &lt;code&gt;application.properties&lt;/code&gt; file for the message bridge, with the following contents:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;#amq6 activemq.source.brokerURL=ssl://test-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443 activemq.source.username=admin activemq.source.password=admin activemq.source.ssl.keyStorePassword=password activemq.source.ssl.keyStoreLocation=/etc/ssl/example-amq6-secret/broker.ks activemq.source.ssl.trustStorePassword=password activemq.source.ssl.trustStoreLocation=/etc/ssl/example-amq6-secret/broker.ts activemq.source.type=queue activemq.source.name=test #amq7 quarkus.qpid-jms.url=amqps://example-amq7-amqp-0-svc-rte-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443?transport.keyStoreLocation=/etc/ssl/example-amq-secret/broker.ks&amp;transport.keyStorePassword=password&amp;transport.trustStoreLocation=/etc/ssl/example-amq-secret/client.ts&amp;transport.trustStorePassword=password&amp;transport.verifyHost=false quarkus.qpid-jms.username=admin quarkus.qpid-jms.password=admin jms.destinationType=queue jms.destinationName=test&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the &lt;code&gt;message-bridge&lt;/code&gt; folder, create a program named &lt;code&gt;sixToSevenBridge.java&lt;/code&gt;, with the following contents:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt; @Override public void configure() throws Exception { from("activeMQSource:{{activemq.source.type}}:{{activemq.source.name}}").to("log:info") .to("jms:{{jms.destinationType}}:{{jms.destinationName}}?connectionFactory=artemisConnectionFactory"); } @BindToRegistry("artemisConnectionFactory") public JmsConnectionFactory connectionFactory() throws Exception { return new JmsConnectionFactory(destinationBrokerURL); } @BindToRegistry("activeMQSource") public ActiveMQComponent activeMQSource() throws Exception{ ActiveMQComponent activeMqComponent = new ActiveMQComponent(); activeMqComponent.setConnectionFactory(pooledConnectionFactorySource()); activeMqComponent.setCacheLevelName("CACHE_CONSUMER"); return activeMqComponent; } @BindToRegistry("pooledConnectionFactorySource") public PooledConnectionFactory pooledConnectionFactorySource() throws Exception { return new PooledConnectionFactory(sslConnectionFactorySource()); } private ActiveMQSslConnectionFactory sslConnectionFactorySource() throws Exception { ActiveMQSslConnectionFactory connectionFactory = new ActiveMQSslConnectionFactory(); System.out.println("BrokerURL: " + sourceBrokerURL); connectionFactory.setBrokerURL(sourceBrokerURL); connectionFactory.setUserName(sourceUserName); connectionFactory.setPassword(sourcePassword); connectionFactory.setTrustStore(sourceTrustStoreLocation); connectionFactory.setTrustStorePassword(sourceTruststorePassword); connectionFactory.setKeyStore(sourceKeyStoreLocation); connectionFactory.setKeyStorePassword(sourceKeystorePassword); return connectionFactory; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the message bridge:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kamel run -n camelk-ssl \ --property file:./message-bridge/configs/application.properties message-bridge/sixToSevenBridge.java \ --resource secret:example-amq6-secret@/etc/ssl/example-amq6-secret \ --resource secret:example-amq-secret@/etc/ssl/example-amq-secret &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;View the logs to see whether the message bridge worked:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc logs -f {AMQ7 consumer pod} &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Clean up&lt;/h2&gt; &lt;p&gt;Uninstall the operators using the OpenShift command line:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc delete project camelk-ssl&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article showed you how to use Camel K to connect different versions of the ActiveMQ message broker using Red Hat AMQ on OpenShift 4. Camel K is a highly effective integration framework that runs natively on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. This article showcased just a few of its capabilities. Camel K allows you to build and &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerize&lt;/a&gt; applications easily and quickly. Its integrations use very lightweight pods, thus consuming fewer resources.&lt;/p&gt; &lt;p&gt;Get more information from the &lt;a href="https://camel.apache.org/camel-k/next/"&gt;Camel K documentation&lt;/a&gt;. You might also enjoy the article &lt;a href="https://developers.redhat.com/blog/2020/05/12/six-reasons-to-love-camel-k"&gt;Six reasons to love Camel K&lt;/a&gt; and our series of related &lt;a href="https://developers.redhat.com/courses/camel-k"&gt;Camel K learning courses&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/01/12/integrate-apache-activemq-brokers-using-camel-k" title="Integrate Apache ActiveMQ brokers using Camel K"&gt;Integrate Apache ActiveMQ brokers using Camel K&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Maz Arslan, Anton Giertli</dc:creator><dc:date>2022-01-12T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.15.0 released!</title><link rel="alternate" href="https://blog.kie.org/2022/01/kogito-1-15-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2022/01/kogito-1-15-0-released.html</id><updated>2022-01-12T06:36:03Z</updated><content type="html">We are glad to announce that the Kogito 1.15.0 release is now available! This goes hand in hand with, . From a feature point of view, we included a series of new features and bug fixes, including: * Quarkus Dev Service for Data Index, see * Enhanced support for multi instance sub-process. * Support using Quarkus config mecanish, including profiles and YAML files. * Serverless Workflow support for Workflow Compensation. * Async execution support for process nodes BREAKING CHANGES * Data Index PostgreSQL schema changes, please review release notes for more information. For more details head to the complete. All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found. * Kogito images are available on. * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.15.0 artifacts are available at the. A detailed changelog for 1.15.0 can be found in. New to Kogito? Check out our website. Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title type="html">Intelligent data as a service (iDaaS) - Common architectural elements</title><link rel="alternate" href="http://www.schabell.org/2021/12/idaas-common-architectural-elements.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2021/12/idaas-common-architectural-elements.html</id><updated>2022-01-12T06:00:00Z</updated><content type="html">Part 2 - Common architectural elements In  from this series we introduced an architecture around intelligent data as a service (iDaaS) for the healthcare industry. The process was laid out how we approached the use case and how portfolio solutions are the base for researching a generic architecture.  The only thing left to cover was the order in which you'll be led through the details. This article starts the real journey at the very top, with a generic architecture from which we'll discuss the common architectural elements one by one. ARCHITECTURE REVIEW As mentioned before, the architectural details covered here are based on real solutions using open source technologies. The example scenario presented here is a generic common architecture that was uncovered while researching those solutions. It's our intent to provide guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architecture, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a look at the details in this architecture and outline the solution. FROM SPECIFIC TO GENERIC Before diving into the common elements, it might be nice to understand that this is not a catch all for every possible solution. It's a collection of identified elements that we've uncovered in multiple customer implementations. These elements presented here are then the generic common architectural elements that we've identified and collected into the generic architecture.  It's our intent to provide an architecture for guidance and not deep technical details. You're smart enough to figure out wiring integration points in your own architectures. You're capable of slotting in the technologies and components you've committed to in the past where applicable.  It's our job here to describe the architectural generic components and outline a few specific cases with visual diagrams so that you're able to make the right decisions from the start of your own projects. Another challenge has been how to visually represent the architecture. There are many ways to represent each element, but we've chosen some icons, text and colours that we hope are going to make it all easy to absorb. Now let's take a quick tour of the generic architecture and outline the common elements uncovered in our research. EDGE SERVICES Starting on the left side of the diagram, which is by no means a geographical necessity, there are two elements that represent external edge services that are integrated with the core elements of this architecture.  The first are edge devices, covering basically everything that is used in the field from clinical personnel, patients, to partnering vendors. These can be anything from sensor devices to mobile units such as phones or tablets, but certainly not limited to just these. It's a grouping to identify functionality on the edge of this use case. The second is a catch all for data collection and consumption called external data sources, a broad element containing all of the remote devices. For example, a heart monitoring machine capturing diagnostic information from a heart patient at home. IDAAS CORE These elements are core to the iDaaS solution and are deployed on a container platform to indicate the cloud-ready nature of this architecture.   Starting on the top right and working down from left to right, the first element is iDaaS knowledge insight. This represents the management applications that provide analytics and insights into the data available across the live platform. This can be setup to provide near-realtime gathering and reporting as organisational need require. This architecture relies on event streaming to react and respond to events within the iDaaS solution, making the iDaaS connect events element central to the solution. This is the element designed for collecting data streams and messages, making it all available to applications and services as subscribers, and pushing notifications as needed to interested parties when certain events happen. Core to any data architecture has to be integration. The iDaaS connect element represents the collection of both integration services and data integration services that are essential to bringing data, messages, and systems throughout a healthcare organisation together in a seamless fashion. The iDaaS connect data distribution element is a solution where routing of data to the desired destination happens. This can be incoming data being routed to the correct data store, or out bound data being routed to the right service, application, or end user. One of the most essential requirements for any healthcare organisation is to maintain compliancy to national laws, data privacy, and other transitional requirements. The iDaaS knowledge conformance element is a set of applications and tools that allow for any organisation to automate compliancy and regulation adherence using rule systems customised to their own local needs. This is a very flexible element that can be designed to ensure that compliancy audit requirements are constantly met. While data distribution can be a simple way to ensure data ends up in the right location, often there is a need for a more intelligent near real-time adjustment to the route data needs to take. Using the iDaaS intelligent data router allows for rule based intelligent routing based on message content, data origin, data destinations, or any other metric required to determine routing. For any healthcare organisation one can expect that there are some complexer processes that might require partially automated processing or even human interaction such as the . An element shown here as the iDaaS event builder is meant for capturing all the process automation needs for any healthcare organisation. It's easy to integrate with data, events, services, applications, and provides a treasure trove of processing metrics to help tune your organisations activities for patients and clinical staff. Finally, there is a need to provide access to internal services through generic application programming interfaces, or API's. The API management element provides all the needed functionality to help expose and connect with services, applications, and more. EXTERNAL SERVICES External services host an array of potential tools, systems, or third-party applications that are used in healthcare organisations. Every organisation has reporting services, either internal or external. These can be Software as a Service (SaaS) tools or just hosted tooling that is external to the organisation. There are any number of big data solutions and tools that can be found in healthcare architectures, often external to the organisation as they are working on very, very large data sets.  Monitoring &amp;amp; logging tools are in abundance together with analytics these also can be hosted externally applications, tooling, or interfaces to be integrated into a healthcare architecture. Many organisations are engaged with other partner data services and need to be able to interact in a timely fashion when sharing or pulling from these data sources. All of the above external services have a need to provide consistent access, which is done using an external API management element shown here that can be a SaaS offering or just hosted externally to the organisation. Finally, both security and any DevOps infrastructure or tooling can be hosted externally and needs to be integrated into the delivery processes for healthcare organisations that make use of them. WHAT'S NEXT This was just a short overview of the common generic elements that make up our architecture for the intelligent data as a service architecture.  An overview of this series on intelligent data as a service architecture: 1. 2. 3. Example data architecture 4. Example HL7 and FHIR integration 5. Example iDaaS knowledge and insights Catch up on any articles you missed by following one of the links above. Next in this series, taking a look at an example iDaaS architecture for the intelligent data as a service solution.</content><dc:creator>Eric D. Schabell</dc:creator></entry></feed>
